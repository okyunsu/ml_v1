from konlpy.tag import Okt
import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter
import os
import logging

logger = logging.getLogger("nlp_api")

class SamsungReport:
    def __init__(self):
        self.text = ""
        self.tokens = []
        self.nouns = []
        self.stopwords = []
        self.word_count = {}
        print("ğŸš€ SamsungReport ì´ˆê¸°í™” ì™„ë£Œ")

    def read_file(self):
        print("ğŸ“– íŒŒì¼ ì½ê¸° ì‹œì‘...")
        file_path = os.path.join('app', 'orginal', 'kr-Report_2018.txt')
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                self.text = f.read()
            print(f"âœ… íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(self.text)} ê¸€ì")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            self.text = ""
        return self.text

    def extract_hangul(self):
        print("ğŸ” í•œê¸€ ì¶”ì¶œ ì‹œì‘...")
        # í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°
        original_length = len(self.text)
        self.text = re.sub('[^ã„±-ã…ã…-ã…£ê°€-í£ ]', '', self.text)
        print(f"âœ… í•œê¸€ ì¶”ì¶œ ì™„ë£Œ: {original_length} -> {len(self.text)} ê¸€ì")
        return self.text

    def change_token(self):
        print("ğŸ”„ í† í°í™” ì‹œì‘...")
        # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬
        self.tokens = self.text.split()
        print(f"âœ… í† í°í™” ì™„ë£Œ: {len(self.tokens)} ê°œì˜ í† í°")
        return self.tokens

    def extract_noun(self):
        print("ğŸ“ í˜•íƒœì†Œ ë¶„ì„ ì‹œì‘...")
        okt = Okt()
        self.nouns = []
        total_tokens = len(self.tokens)
        
        for i, token in enumerate(self.tokens, 1):
            if i % 100 == 0:
                print(f"â³ í˜•íƒœì†Œ ë¶„ì„ ì§„í–‰ì¤‘: {i}/{total_tokens} ({i/total_tokens*100:.1f}%)")
            # token ë‚´ ë‹¨ì–´ë³„ í’ˆì‚¬ í™•ì¸
            pos_tags = okt.pos(token)
            # ëª…ì‚¬ë§Œ ì¶”ì¶œ
            self.nouns.extend([word for word, tag in pos_tags if tag == 'Noun'])
        
        print(f"âœ… í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ: {len(self.nouns)} ê°œì˜ ëª…ì‚¬ ì¶”ì¶œ")
        return self.nouns

    def read_stopword(self):
        print("ğŸ“š ë¶ˆìš©ì–´ ì‚¬ì „ ì½ê¸° ì‹œì‘...")
        # stopwords.txt íŒŒì¼ì—ì„œ ë¶ˆìš©ì–´ ì½ê¸°
        file_path = os.path.join('app', 'orginal', 'stopwords.txt')
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                # íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ì™€ì„œ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
                words = f.read().strip().split()
                # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ê°€ 1ì¸ ë‹¨ì–´ ì œì™¸
                self.stopwords = list(set([word for word in words if len(word) > 1]))
            print(f"âœ… ë¶ˆìš©ì–´ ì‚¬ì „ ì½ê¸° ì™„ë£Œ: {len(self.stopwords)} ê°œì˜ ë¶ˆìš©ì–´")
        except Exception as e:
            print(f"âŒ ë¶ˆìš©ì–´ ì‚¬ì „ ì½ê¸° ì˜¤ë¥˜: {e}")
            # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë¶ˆìš©ì–´ ì„¤ì •
            self.stopwords = ['ë°', 'ë“±', 'ãˆœ', 'ê²ƒ', 'ì €', 'ì›”', 'ë…„', 'ë¶„ê¸°', 'ì–µì›', 'ì¡°ì›']
            print("âš ï¸ ê¸°ë³¸ ë¶ˆìš©ì–´ ì„¤ì • ì ìš©")
        
        return self.stopwords

    def remove_stopword(self):
        print("ğŸ—‘ï¸ ë¶ˆìš©ì–´ ì œê±° ì‹œì‘...")
        if not self.stopwords:
            self.read_stopword()

        # 1. ê¸°ë³¸ í•„í„°ë§
        filtered = [noun for noun in self.nouns if len(noun) > 1 and not noun.isdigit()]
        print(f"1ï¸âƒ£ ê¸°ë³¸ í•„í„°ë§ ì™„ë£Œ: {len(self.nouns)} -> {len(filtered)}")

        # 2. ë¹ˆë„ ê¸°ë°˜ ìë™ í•„í„°ë§
        counter = Counter(filtered)
        total = sum(counter.values())
        max_freq_ratio = 0.03
        min_freq = 3
        auto_stopwords = {
            word for word, freq in counter.items()
            if freq / total >= max_freq_ratio or freq < min_freq
        }
        print(f"2ï¸âƒ£ ë¹ˆë„ ê¸°ë°˜ í•„í„°ë§: {len(auto_stopwords)} ê°œì˜ ë‹¨ì–´ ì œê±° ì˜ˆì •")

        # 3. ì˜ë¯¸ê¸°ë°˜ ë¶ˆìš©ì–´ ì¶”ê°€
        SEMANTIC_STOPWORDS = {
            "ê°ì£¼", "ì‚¬ì˜", "ì œì‹œ", "ê°€ëŠ¥", "ì‚¬í•­", "ë‚´ìš©", "ê²½ìš°", "ì¶œì—°", "ì—°ì¥",
            "ë¶€ë¶„", "ëŒ€ìƒ", "ìƒí™©", "ìˆ˜ì¤€", "ê´€ë ¨", "ì¡°ì¹˜", "ê²°ê³¼", "ì´ìƒ", "ì²´ê³„",
            "ë°©ì•ˆ", "ì‚¬ì•ˆ", "ëŒ€ë¹„", "ê¸°ì¤€", "ëŒ€ì‘", "í™•ë³´", "ì‹¤í˜„", "ìˆ˜ë¦½", "ê³ ë ¤",
            "ê³„íš", "í•­ëª©", "ê¸°íƒ€", "í¬í•¨", "ì¡°ì •", "ì—°ê³„", "ì ìš©", "ê²€í† "
        }

        all_stopwords = set(self.stopwords) | auto_stopwords | SEMANTIC_STOPWORDS
        self.nouns = [noun for noun in filtered if noun not in all_stopwords]

        print(f"âœ… ë¶ˆìš©ì–´ ì œê±° ì™„ë£Œ: {len(filtered)} -> {len(self.nouns)} ê°œì˜ ë‹¨ì–´")
        print(f"[ìë™ ì œê±°ëœ ë¶ˆìš©ì–´ ì˜ˆì‹œ] {list(auto_stopwords)[:10]}")
        return self.nouns

    def find_frequency(self):
        print("ğŸ“Š ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘...")
        # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
        self.word_count = Counter(self.nouns)
        result = dict(self.word_count.most_common(100))
        print(f"âœ… ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì™„ë£Œ: ìƒìœ„ 10ê°œ ë‹¨ì–´ {list(result.items())[:10]}")
        return result

    def draw_wordcloud(self):
        print("ğŸ¨ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹œì‘...")
        # matplotlib ë°±ì—”ë“œë¥¼ Aggë¡œ ì„¤ì • (GUI ì—†ì´ ë™ì‘)
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        
        try:
            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            wc = WordCloud(
                font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  # ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ê²½ë¡œ
                background_color='white',
                width=800,
                height=600,
                max_words=100,
                max_font_size=200
            )
            print("1ï¸âƒ£ ì›Œë“œí´ë¼ìš°ë“œ ê°ì²´ ìƒì„± ì™„ë£Œ")
            
            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            wc.generate_from_frequencies(self.word_count)
            print("2ï¸âƒ£ ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ìƒì„± ì™„ë£Œ")
            
            # ì´ë¯¸ì§€ ì €ì¥ - app/output í´ë”ì— ì €ì¥
            output_dir = os.path.join('app', 'output')
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
                print(f"3ï¸âƒ£ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")
            
            plt.figure(figsize=(10, 8))
            plt.imshow(wc, interpolation='bilinear')
            plt.axis('off')
            
            output_path = os.path.join(output_dir, 'samsung_wordcloud.png')
            plt.savefig(output_path)
            plt.close('all')  # ëª¨ë“  figure ë‹«ê¸°
            
            print(f"âœ… ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            raise e
